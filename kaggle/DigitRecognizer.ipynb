{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras.callbacks as cb\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PreprocessDataset():\n",
    "    # The competition datafiles are in the directory ../input\n",
    "    # Read competition data files:\n",
    "    train = pd.read_csv(\"./input/train.csv\")\n",
    "    test  = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "    # Write to the log:\n",
    "    print(\"Training set has {0[0]} rows and {0[1]} columns\".format(train.shape))\n",
    "    print(\"Test set has {0[0]} rows and {0[1]} columns\".format(test.shape))\n",
    "    # Any files you write to the current directory get shown as outputs\n",
    "    \n",
    "    if type(train) is pd.core.frame.DataFrame:\n",
    "        train = train.as_matrix()\n",
    "\n",
    "\n",
    "    if type(test) is pd.core.frame.DataFrame:\n",
    "        test = test.as_matrix()\n",
    "\n",
    "    x_train = train[:,1:]\n",
    "    y_train = train[:,:1]\n",
    "\n",
    "    x_test = test[:,1:]\n",
    "    y_test = test[:,:1]\n",
    "    \n",
    "    ## Transform labels to one-hot encoding\n",
    "    ## i.e., from '7' to [0,0,0,0,0,0,0,1,0,0]\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    ## Process features. Set numeric type\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # Activity 1 (Pre-processing):\n",
    "    # Group A: w/o pre-processing datasets.\n",
    "    #\n",
    "    # Group B: Min-Max Normalize value to [0, 1]\n",
    "    # x_train /= 255\n",
    "    # x_test /= 255\n",
    "    #\n",
    "    # Group C: proceed w/ standardizing datasets by z-scoring (de-mean, uni-variance).\n",
    "    # x_train = preprocessing.scale(x_train)\n",
    "    # x_test = preprocessing.scale(x_test)\n",
    "    ################################################################  \n",
    "    ## YOUR TURN: CHANGE HERE\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 42000 rows and 785 columns\n",
      "Test set has 28000 rows and 784 columns\n",
      "x_train type: (42000, 784)\n",
      "x_test type: (28000, 783)\n",
      "y_train type: (42000, 10)\n",
      "y_test type: (28000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = PreprocessDataset()\n",
    "print(\"x_train type: \" + str(x_train.shape))\n",
    "print(\"x_test type: \" + str(x_test.shape))\n",
    "print(\"y_train type: \" + str(y_train.shape))\n",
    "print(\"y_test type: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     x                      |  y  \n",
      "==================================================\n",
      "0.00 0.00 ... 0.00 0.00 0.00 ...  0.00 0.00 |  0\n",
      "0.00 0.00 ... 0.96 0.59 0.13 ...  0.00 0.00 |  1\n",
      "0.00 0.00 ... 0.00 0.00 0.00 ...  0.00 0.00 |  0\n",
      "0.00 0.00 ... 0.00 0.04 0.30 ...  0.00 0.00 |  0\n",
      "0.00 0.00 ... 0.99 0.99 0.45 ...  0.00 0.00 |  1\n",
      "0.00 0.00 ... 0.98 0.96 0.28 ...  0.00 0.00 |  1\n",
      "0.00 0.00 ... 0.00 0.00 0.00 ...  0.00 0.00 |  0\n",
      "0.00 0.00 ... 0.00 0.00 0.00 ...  0.00 0.00 |  0\n",
      "0.00 0.00 ... 0.70 1.00 1.00 ...  0.00 0.00 |  0\n",
      "0.00 0.00 ... 0.99 0.49 0.00 ...  0.00 0.00 |  0\n"
     ]
    }
   ],
   "source": [
    "## Show part of training data: features and labels\n",
    "## Each row is a sample, and each column represents a feature.\n",
    "print(\"{:^43}\".format(\"x\"), \"|\", \"{:^4}\".format(\"y\"))\n",
    "print(\"=\"*50)\n",
    "for sample_id in range(10):\n",
    "    print(\"{:.2f} {:.2f} ... {:.2f} {:.2f} {:.2f} ...  {:.2f} {:.2f}\".format(\n",
    "            x_train[sample_id][0], x_train[sample_id][1],\n",
    "            x_train[sample_id][156], x_train[sample_id][157], x_train[sample_id][158],\n",
    "            x_train[sample_id][-2], x_train[sample_id][-1]), \"| \",\n",
    "           \"{:.0f}\".format(y_train[sample_id][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DefineModel():\n",
    "\n",
    "    ################################################################\n",
    "    # Activity 2 (Network Structure):\n",
    "    # Group A: uses only 1 layer\n",
    "    # second_layer_width = 0\n",
    "    #\n",
    "    # Group B: uses 2 layers of a tower-shaped (same width) network.\n",
    "    # second_layer_width = 128\n",
    "    #\n",
    "    # Group C: uses 2 layers of a pyramid-shaped (shrink width) network.\n",
    "    # second_layer_width = 64\n",
    "    ################################################################\n",
    "    first_layer_width = 128\n",
    "    second_layer_width = 64   \n",
    "    \n",
    "    ################################################################\n",
    "    # Activity 3 (Activation Function):\n",
    "    # Group A uses ReLU.\n",
    "    # activation_func = 'relu' \n",
    "    # \n",
    "    # Group B uses Sigmoid.\n",
    "    # activation_func = 'sigmoid'\n",
    "    #\n",
    "    # Group C uses Tanh.\n",
    "    # activation_func = 'tanh'\n",
    "    ################################################################\n",
    "    activation_func = 'relu' \n",
    "\n",
    "    ################################################################    \n",
    "    # Activity 4 (Loss Function):\n",
    "    # Group A uses cross entropy.\n",
    "    # loss_function = 'categorical_crossentropy'\n",
    "    # \n",
    "    # Group B uses cross entropy.\n",
    "    # loss_function = 'categorical_crossentropy'\n",
    "    # \n",
    "    # Group C uses squared error.\n",
    "    # loss_function = 'mean_squared_error'\n",
    "    ################################################################    \n",
    "    loss_function = 'categorical_crossentropy'\n",
    "    \n",
    "    #################################################################    \n",
    "    # Activity 5 (Dropout):\n",
    "    # Group A uses 0% dropout.\n",
    "    #\n",
    "    # Group B uses 50% dropout.\n",
    "    # dropout_rate = 0.5\n",
    "    #\n",
    "    # Group C uses 90% dropout.\n",
    "    # dropout_rate = 0.9\n",
    "    #################################################################    \n",
    "    dropout_rate = 0\n",
    "    \n",
    "    ################################################################    \n",
    "    # Activity 6 (Regularization):\n",
    "    # Group A uses L1 regularizer\n",
    "    # weight_regularizer = l1(0.01)\n",
    "    #\n",
    "    # Group B uses L2 regularizer\n",
    "    # weight_regularizer = l2(0.01)\n",
    "    # \n",
    "    # Group C uses no regularizer\n",
    "    # weight_regularizer = None\n",
    "    ################################################################\n",
    "    weight_regularizer = l2(0.01)\n",
    "\n",
    "    ################################################################    \n",
    "    # Activity 8 (Learning Rate):\n",
    "    # Group A uses learning rate of 0.1.\n",
    "    # learning_rate = 0.1\n",
    "    # \n",
    "    # Group B uses learning rate of 0.01.\n",
    "    # learning_rate = 0.01\n",
    "    #\n",
    "    # Group C uses learning rate of 0.5.    \n",
    "    # learning_rate = 0.5\n",
    "    ################################################################\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    ## Initialize model.\n",
    "    model = Sequential()\n",
    "\n",
    "    ## First hidden layer with 'first_layer_width' neurons. \n",
    "    ## Also need to specify input dimension.\n",
    "    ## 'Dense' means fully-connected.\n",
    "    model.add(Dense(first_layer_width, input_dim=784, W_regularizer=weight_regularizer))\n",
    "    model.add(Activation(activation_func))\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    ## Second hidden layer.\n",
    "    if second_layer_width > 0:\n",
    "        model.add(Dense(second_layer_width))\n",
    "        model.add(Activation(activation_func))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(0.5))         \n",
    "    \n",
    "    ## Last layer has the same dimension as the number of classes\n",
    "    ## For classification, the activation is softmax\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    ## Define optimizer. In this tutorial/codelab, we select SGD.\n",
    "    ## You can also use other methods, e.g., opt = RMSprop()\n",
    "    opt = SGD(lr=learning_rate, clipnorm=5.)\n",
    "    ## Define loss function = 'categorical_crossentropy' or 'mean_squared_error'\n",
    "    model.compile(loss=loss_function, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrainModel(data=None, epochs=20):\n",
    "    ################################################################\n",
    "    # Activity 7 (Mini-batch):\n",
    "    # Group A uses mini-batch of size 128.\n",
    "    # batch = 128\n",
    "    #\n",
    "    # Group B uses mini-batch of size 256.\n",
    "    # batch = 256\n",
    "    # \n",
    "    # Group C uses mini-batch of size 512.\n",
    "    # batch = 512\n",
    "    ################################################################\n",
    "    batch=128\n",
    "    start_time = time.time()\n",
    "    model = DefineModel()\n",
    "    if data is None:\n",
    "        print(\"Must provide data.\")\n",
    "        return\n",
    "    x_train, x_test, y_train, y_test = data\n",
    "    print('Start training.')\n",
    "    ## Use the first 55,000 (out of 60,000) samples to train, last 5,500 samples to validate.\n",
    "    history = model.fit(x_train[:55000], y_train[:55000], nb_epoch=epochs, batch_size=batch,\n",
    "              validation_data=(x_train[55000:], y_train[55000:]))\n",
    "    print(\"Training took {0} seconds.\".format(time.time() - start_time))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "Train on 42000 samples, validate on 0 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 2s - loss: 1.8157 - acc: 0.8568     \n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.6752 - acc: 0.9258     \n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.3984 - acc: 0.9355     \n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.3097 - acc: 0.9428     \n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2707 - acc: 0.9497     \n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2544 - acc: 0.9520     \n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2444 - acc: 0.9538     \n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 2s - loss: 0.2327 - acc: 0.9577     \n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2304 - acc: 0.9574     \n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2344 - acc: 0.9557     \n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 2s - loss: 0.2225 - acc: 0.9602     \n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 2s - loss: 0.2272 - acc: 0.9582     \n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2215 - acc: 0.9606     \n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2152 - acc: 0.9614     \n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2157 - acc: 0.9613     \n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2125 - acc: 0.9620     \n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2204 - acc: 0.9611     \n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2148 - acc: 0.9618     \n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2114 - acc: 0.9636     \n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 1s - loss: 0.2140 - acc: 0.9627     \n",
      "Training took 39.47049307823181 seconds.\n"
     ]
    }
   ],
   "source": [
    "trained_model, training_history = TrainModel(data=[x_train, x_test, y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
